{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import queue\n",
    "import logging\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import datetime\n",
    "import binascii\n",
    "from typing import MutableSequence\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from dask.utils import natural_sort_key\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from airsecoma import azure_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_utils.init_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 10:10:02 - INFO - diff.ipnb - Logging test INFO\n"
     ]
    }
   ],
   "source": [
    "azure_utils.init_logging()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger('azure').setLevel(logging.WARNING)\n",
    "log = logging.getLogger(\"diff.ipnb\")\n",
    "log.info(\"Logging test INFO\")\n",
    "log.debug(\"Logging test DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENANT_ID = \"46b696da-8827-46db-8cc7-b5020d3f4056\"\n",
    "FS_CONFS = {\n",
    "    \"dev\": { \"account\": \"skyguidecapmandatadev\", \"linked_service\": \"skyguidecapmandatadev\", \"tenant_id\": TENANT_ID },\n",
    "    \"uat\": { \"account\": \"skyguidecapmandatauat\", \"linked_service\": \"skyguidecapmandatauat\", \"tenant_id\": TENANT_ID },\n",
    "    \"prd\": { \"account\": \"skyguidecapmandataprd\", \"linked_service\": \"skyguidecapmandataprd\", \"tenant_id\": TENANT_ID },\n",
    "}\n",
    "FS_CONF_BACKUP_DATA     = { \"account\": \"skyguidecapmanbackup\", \"linked_service\": \"skyguidecapmanbackup\", \"tenant_id\": TENANT_ID }\n",
    "FS_CONF_BACKUP_METADATA = { \"account\": \"skyguidecapmanbackup\", \"linked_service\": \"skyguidecapmanbackup\", \"tenant_id\": TENANT_ID }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATIONS_TO_BACKUP = [\n",
    "    { \"fs_name\": \"dev\", \"root\": \"skyguide-data\" },\n",
    "    { \"fs_name\": \"uat\", \"root\": \"skyguide-data\" },\n",
    "    { \"fs_name\": \"prd\", \"root\": \"skyguide-data\" },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKUP_DATA_FOLDER = \"data\"\n",
    "BACKUP_METADATA_FOLDER = \"metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_by_name = {\n",
    "    name: azure_utils.init_azure_fs(**config)\n",
    "    for name, config in FS_CONFS.items()\n",
    "}\n",
    "fs_backup_data = azure_utils.init_azure_fs(**FS_CONF_BACKUP_DATA)\n",
    "fs_backup_metadata = azure_utils.init_azure_fs(**FS_CONF_BACKUP_METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoneType = type(None)\n",
    "def normalize_details(v):\n",
    "    if isinstance(v, (str, int, float, bool, NoneType)):\n",
    "        return v\n",
    "    elif isinstance(v, (datetime.datetime,datetime.date)):\n",
    "        return v.isoformat()\n",
    "    elif isinstance(v, bytearray):\n",
    "        return binascii.hexlify(v).decode('ascii')\n",
    "    elif hasattr(v, \"items\") and callable(v.items) and hasattr(v, \"__getitem__\") and callable(v.__getitem__):\n",
    "        return {k: normalize_details(v) for k, v in v.items()}\n",
    "    elif isinstance(v, list) or isinstance(v, MutableSequence) or (hasattr(v, \"__iter__\") and callable(v.__iter__)):\n",
    "        return [normalize_details(v) for v in v]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type '{type(v).__module__}.{type(v).__name__}' for instance {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collector:\n",
    "    def __init__(self, fs):\n",
    "        self.fs = fs\n",
    "        self.is_cancelled = False\n",
    "        self.folders_to_process = queue.Queue()\n",
    "        self.collected_files = queue.Queue()\n",
    "        self.collected_errors = queue.Queue()\n",
    "\n",
    "    def collect(self):\n",
    "        try:\n",
    "            while True:\n",
    "                if self.is_cancelled:\n",
    "                    raise Exception(\"Cancelled\")\n",
    "                try:\n",
    "                    folder = self.folders_to_process.get(block=True, timeout=1)\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    details = self.fs.ls(folder, detail=True)\n",
    "                    for d in details:\n",
    "                        if d['type'] == 'directory':\n",
    "                            self.folders_to_process.put(d['name'])\n",
    "                        else:\n",
    "                            self.collected_files.put(d)\n",
    "                finally:\n",
    "                    self.folders_to_process.task_done()\n",
    "        except Exception as e:\n",
    "            self.collected_errors.put(e)\n",
    "\n",
    "    def parallel_collect(self, parallel=20, log_every_sec=10):\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=parallel) as executor:\n",
    "            # start the workers\n",
    "            for _ in range(parallel):\n",
    "                executor.submit(self.collect)\n",
    "\n",
    "            # wait for the workers to finish\n",
    "            try:\n",
    "                last_log_time = time.monotonic()\n",
    "                while True:\n",
    "                    if self.folders_to_process.unfinished_tasks == 0:\n",
    "                        log.info(f\"collect_all_files_recursive: Finished collecting files\")\n",
    "                        break\n",
    "                    if self.collected_errors.qsize() > 0:\n",
    "                        raise Exception(f\"Error while collecting files: {self.collected_errors.get()}\")\n",
    "\n",
    "                    now = time.monotonic()\n",
    "                    if now - last_log_time > log_every_sec:\n",
    "                        last_log_time = now\n",
    "                        log.info(f\"collect_all_files_recursive: collected_files={self.collected_files.qsize()}, folders_to_process={self.folders_to_process.qsize()}\")\n",
    "                    time.sleep(1)\n",
    "            finally:\n",
    "                self.is_cancelled = True\n",
    "                log.info(f\"END OF collect_all_files_recursive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def _build_backup_path(md5):\n",
    "    return f\"{BACKUP_DATA_FOLDER}/{md5[:2]}/{md5[2:4]}/{md5[4:]}/{md5}.gz\"\n",
    "\n",
    "class Migrator:\n",
    "    def __init__(\n",
    "        self, \n",
    "        fs_by_name, \n",
    "        fs_backup_data, \n",
    "        backup_data_folder, \n",
    "        fs_backup_metadata, \n",
    "        backup_metadata_folder,\n",
    "        parallel=20, \n",
    "        log_every_sec=10,\n",
    "    ):\n",
    "        self.fs_by_name             = fs_by_name\n",
    "        self.fs_backup_data         = fs_backup_data\n",
    "        self.backup_data_folder     = backup_data_folder\n",
    "        self.fs_backup_metadata     = fs_backup_metadata\n",
    "        self.backup_metadata_folder = backup_metadata_folder\n",
    "        self.parallel               = parallel\n",
    "        self.log_every_sec          = log_every_sec\n",
    "        self._backup_data_snapshot  = None\n",
    "\n",
    "    @property\n",
    "    def backup_data_snapshot(self):\n",
    "        if self._backup_data_snapshot is None:\n",
    "            log.log(f\"backup_data_snapshot: loading backup data snapshot\")\n",
    "            self._backup_data_snapshot = self._get_backup_data_snapshot()\n",
    "        return self._backup_data_snapshot\n",
    "        \n",
    "    def get_snapshot(self, fs_name, folder):\n",
    "        \"\"\"Get the current snapshot of a folder in a filesystem\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: DataFrame with columns name, size, md5 (dtypes: string, int64, string)\n",
    "        \"\"\"\n",
    "        fs = self.fs_by_name[fs_name]\n",
    "        c = Collector(fs)\n",
    "        c.folders_to_process.put(folder)\n",
    "        c.parallel_collect(parallel=self.parallel, log_every_sec=self.log_every_sec)\n",
    "        cf = normalize_details(c.collected_files.queue)\n",
    "        if len(cf) == 0:\n",
    "            return pd.DataFrame(\n",
    "                columns=['name', 'size', 'md5'], \n",
    "                dtypes=['string', 'int64', 'string']\n",
    "            )\n",
    "\n",
    "        df = pd.json_normalize(cf)\n",
    "        df = df.rename(columns={ 'content_settings.content_md5': 'md5' })\n",
    "        df = df[['name', 'size', 'md5']]\n",
    "        \n",
    "        # remove folder prefix\n",
    "        df['name'] = df['name'].str.replace(rf\"^{re.escape(folder)}/\", \"\", regex=True)\n",
    "        \n",
    "        # coerce dtypes\n",
    "        df['name'] = df['name'].astype('string')\n",
    "        df['size'] = df['size'].astype('int64')\n",
    "        df['md5'] = df['md5'].astype('string')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_backup_snapshot_versions(self, fs_name, folder):\n",
    "        backup_file_paths = fs_backup_metadata.glob(f\"{BACKUP_METADATA_FOLDER}/{fs_name}/{folder}/*.parquet\")\n",
    "        return sorted([os.path.splitext(os.path.basename(p))[0] for p in backup_file_paths], key=natural_sort_key)\n",
    "    \n",
    "    def get_backup_snapshot(self, fs_name, folder, version):\n",
    "        df = pd.read_parquet(f\"{BACKUP_METADATA_FOLDER}/{fs_name}/{folder}/{version}.parquet\", filesystem=fs_backup_metadata)\n",
    "            \n",
    "    def get_snapshot_diff(self, snapshot_src, snapshot_dst):\n",
    "        \"\"\"Perform a diff between two snapshots\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: DataFrame with columns name, size_src, size_dst, md5_src, md5_dst, diff (equal, different, only_src, only_dst)\n",
    "        \"\"\"\n",
    "        df_diff = pd.merge(snapshot_src, snapshot_dst, how='outer', suffixes=('_src', '_dst'), on='name', indicator=True)\n",
    "        df_diff = df_diff.rename(columns={ '_merge': 'diff' })\n",
    "        df_diff.loc[df_diff['diff'] == 'left_only', 'diff'] = 'only_src'\n",
    "        df_diff.loc[df_diff['diff'] == 'right_only', 'diff'] = 'only_dst'\n",
    "        mask = df_diff['diff'] == 'both'\n",
    "        df_diff.loc[mask, 'diff'] = np.where(df_diff.loc[mask, 'md5_src'] == df_diff.loc[mask, 'md5_dst'], 'equal', 'different')\n",
    "        return df_diff\n",
    "    \n",
    "    def do_backup(self, fs_name, folder, backup_version, snapshot=None):\n",
    "        log.info(f\"do_backup: starting backup #'{backup_version}' of {fs_name=}, {folder=}\")\n",
    "\n",
    "        fs = fs_by_name[fs_name]\n",
    "        \n",
    "        if snapshot is None:\n",
    "            snapshot = self.get_snapshot(fs_name, folder)\n",
    "            \n",
    "        # find md5 that must be backed up\n",
    "        files_to_backup = snapshot[snapshot['md5'].isin(self.backup_data_snapshot['md5'])]        \n",
    "        # keep only the first file with a given md5, because we only need to upload one copy of the file\n",
    "        files_to_backup = files_to_backup.drop_duplicates(subset=['md5'], keep='first')\n",
    "\n",
    "        log.info(f\"do_backup: found {len(files_to_backup)} new files to backup\")\n",
    "        \n",
    "        # backup data\n",
    "        with ThreadPoolExecutor(max_workers=self.parallel) as executor:\n",
    "            results = executor.map(lambda row: self._backup_file(fs_name, row['name'], row['md5']), files_to_backup.iterrows())\n",
    "            for _ in results:\n",
    "                pass  # raises exception if any of the threads failed\n",
    "                \n",
    "        # backup metadata\n",
    "        snapshot_path = f\"{BACKUP_METADATA_FOLDER}/{fs_name}/{folder}/{backup_version}.parquet\"\n",
    "        with fs_backup_metadata.open(snapshot_path, 'wb') as f:\n",
    "            snapshot.to_parquet(f)\n",
    "\n",
    "        # update inplace the backup snapshot to include the new backed up files\n",
    "        self._backup_data_snapshot = pd.concat([self._backup_data_snapshot, files_to_backup])\n",
    "\n",
    "    def restore_backup(self, dst_fs_name, df_diff):\n",
    "        df = pd.concat(\n",
    "            df_diff[df_diff['diff'] == 'only_right'],\n",
    "            df_diff[df_diff['diff'] == 'different']\n",
    "        )\n",
    "        log.info(f\"restore_backup: restoring {len(df)} missing or changed files\")\n",
    "        with ThreadPoolExecutor(max_workers=parallel) as executor:\n",
    "            results = executor.map(lambda row: restore_file(fs_name, row['name'], row['md5_right']), df.iterrows())\n",
    "            for _ in results:\n",
    "                pass  # raises exception if any\n",
    "\n",
    "    def _backup_single_file(self, fs_name, name, md5):\n",
    "        fs = self.fs_by_name[fs_name]\n",
    "        backup_file_path = _build_backup_path(md5)\n",
    "        log.info(f\"do_backup: backing up file '{name}' to '{backup_file_path}'\")\n",
    "        # open two handles to the file, one for reading and one for writing\n",
    "        with fs.open(name, 'rb') as f_read, \\\n",
    "                fs_backup_data.open(backup_file_path, 'wb') as f_write, \\\n",
    "                gzip.GzipFile(fileobj=f_write, mode='wb') as gzip_file:\n",
    "            shutil.copyfileobj(f_read, gzip_file)\n",
    "\n",
    "    def _restore_file(self, fs_name, name, md5):\n",
    "        fs = fs_by_name[fs_name]\n",
    "        backup_file_path = _build_backup_path(md5)\n",
    "        log.info(f\"do_backup: restoring file '{name}' from '{backup_file_path}'\")\n",
    "        with fs_backup_data.open(backup_file_path, 'rb') as f_read, \\\n",
    "                fs.open(name, 'wb') as f_write, \\\n",
    "                gzip.GzipFile(fileobj=f_read, mode='rb') as gzip_file:\n",
    "            shutil.copyfileobj(gzip_file, f_write)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def backup_all_locations():\n",
    "    backup_version = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    backup_data_snapshot = get_current_snapshot(fs_backup_data, BACKUP_DATA_FOLDER)\n",
    "    for loc in LOCATIONS_TO_BACKUP:\n",
    "        fs_name = loc['fs_name']\n",
    "        root = loc['root']\n",
    "        backup_data_snapshot = do_backup(fs_name, root, backup_data_snapshot, backup_version)\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_snapshot = get_backup_snapshot(fs_backup, BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = make_diff(curr_snapshot, prev_snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display stats by diff type\n",
    "diff.groupby('diff').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_files = get_current_snapshot(fs_backup, 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backup_path(md5):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_files.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
